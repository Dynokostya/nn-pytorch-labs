{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Компʼютерний практикум №15\n",
    "Виконав студент групи ЗК-41мп Гломозда Костянтин\n",
    "\n",
    "TRANSFER LEARNING (ЧАСТИНА II).\n",
    "ЗБЕРЕЖЕННЯ МОДЕЛІ, ЗАВАНТАЖЕННЯ МОДЕЛІ, ЗМІНА\n",
    "ЗАВАНТАЖЕНОЇ МОДЕЛІ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the CRF class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF(nn.Module):\n",
    "    def __init__(self, n_dice, log_likelihood):\n",
    "        super(CRF, self).__init__()\n",
    "        self.n_states = n_dice\n",
    "        self.transition = torch.nn.init.normal_(nn.Parameter(torch.randn(n_dice, n_dice + 1)), -1, 0.1)\n",
    "        self.loglikelihood = log_likelihood\n",
    "\n",
    "    def to_scalar(self, var):\n",
    "        return var.view(-1).data.tolist()[0]\n",
    "\n",
    "    def argmax(self, vec):\n",
    "        _, idx = torch.max(vec, 1)\n",
    "        return self.to_scalar(idx)\n",
    "\n",
    "    def log_sum_exp(self, vec):\n",
    "        max_score = vec[0, self.argmax(vec)]\n",
    "        max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "        return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "    def _data_to_likelihood(self, rolls):\n",
    "        return Variable(torch.FloatTensor(self.loglikelihood[rolls]), requires_grad=False)\n",
    "\n",
    "    def _compute_likelihood_numerator(self, loglikelihoods, states):\n",
    "        prev_state = self.n_states\n",
    "        score = Variable(torch.Tensor([0]))\n",
    "        for index, state in enumerate(states):\n",
    "            score += self.transition[state, prev_state] + loglikelihoods[index, state]\n",
    "            prev_state = state\n",
    "        return score\n",
    "\n",
    "    def _compute_likelihood_denominator(self, loglikelihoods):\n",
    "        prev_alpha = self.transition[:, self.n_states] + loglikelihoods[0].view(1, -1)\n",
    "        for roll in loglikelihoods[1:]:\n",
    "            alpha_t = []\n",
    "            for next_state in range(self.n_states):\n",
    "                feature_function = self.transition[next_state, :self.n_states].view(1, -1) + \\\n",
    "                                   roll[next_state].view(1, -1).expand(1, self.n_states)\n",
    "                alpha_t_next_state = prev_alpha + feature_function\n",
    "                alpha_t.append(self.log_sum_exp(alpha_t_next_state))\n",
    "            prev_alpha = torch.stack(alpha_t).view(1, -1)\n",
    "        return self.log_sum_exp(prev_alpha)\n",
    "\n",
    "    def _viterbi_algorithm(self, loglikelihoods):\n",
    "        argmaxes = []\n",
    "        prev_delta = self.transition[:, self.n_states].view(1, -1) + loglikelihoods[0].view(1, -1)\n",
    "        for roll in loglikelihoods[1:]:\n",
    "            local_argmaxes = []\n",
    "            next_delta = []\n",
    "            for next_state in range(self.n_states):\n",
    "                feature_function = self.transition[next_state, :self.n_states].view(1, -1) + \\\n",
    "                                   roll.view(1, -1) + prev_delta\n",
    "                most_likely_state = self.argmax(feature_function)\n",
    "                score = feature_function[0][most_likely_state]\n",
    "                next_delta.append(score)\n",
    "                local_argmaxes.append(most_likely_state)\n",
    "            prev_delta = torch.stack(next_delta).view(1, -1)\n",
    "            argmaxes.append(local_argmaxes)\n",
    "        final_state = self.argmax(prev_delta)\n",
    "        final_score = prev_delta[0][final_state]\n",
    "        path_list = [final_state]\n",
    "        for states in reversed(argmaxes):\n",
    "            final_state = states[final_state]\n",
    "            path_list.append(final_state)\n",
    "        return np.array(path_list), final_score\n",
    "\n",
    "    def neg_log_likelihood(self, rolls, states):\n",
    "        loglikelihoods = self._data_to_likelihood(rolls)\n",
    "        states = torch.LongTensor(states)\n",
    "        sequence_loglik = self._compute_likelihood_numerator(loglikelihoods, states)\n",
    "        denominator = self._compute_likelihood_denominator(loglikelihoods)\n",
    "        return denominator - sequence_loglik\n",
    "\n",
    "    def forward(self, rolls):\n",
    "        loglikelihoods = self._data_to_likelihood(rolls)\n",
    "        return self._viterbi_algorithm(loglikelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_train_loop(model, rolls, targets, n_epochs, learning_rate=0.01):\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    for epoch in range(n_epochs):\n",
    "        batch_loss = []\n",
    "        N = rolls.shape[0]\n",
    "        model.zero_grad()\n",
    "        for index, (roll, labels) in enumerate(zip(rolls, targets)):\n",
    "            neg_log_likelihood = model.neg_log_likelihood(roll, labels)\n",
    "            batch_loss.append(neg_log_likelihood)\n",
    "            if index % 50 == 0:\n",
    "                ll = torch.cat(batch_loss).mean()\n",
    "                ll.backward()\n",
    "                optimizer.step()\n",
    "                batch_loss = []\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_dice = np.array([1 / 6] * 6)\n",
    "loaded_dice = np.array([0.04, 0.04, 0.04, 0.04, 0.04, 0.8])\n",
    "probabilities = {'fair': fair_dice, 'loaded': loaded_dice}\n",
    "transition_mat = {'fair': np.array([0.8, 0.2, 0.0]),\n",
    "                  'loaded': np.array([0.35, 0.65, 0.0]),\n",
    "                  'start': np.array([0.5, 0.5, 0.0])}\n",
    "states = ['fair', 'loaded', 'start']\n",
    "state2ix = {'fair': 0, 'loaded': 1, 'start': 2}\n",
    "log_likelihood = np.hstack([np.log(fair_dice).reshape(-1, 1),\n",
    "                            np.log(loaded_dice).reshape(-1, 1)])\n",
    "\n",
    "def simulate_data(n_timesteps):\n",
    "    data = np.zeros(n_timesteps)\n",
    "    prev_state = 'start'\n",
    "    state_list = np.zeros(n_timesteps)\n",
    "    for n in range(n_timesteps):\n",
    "        next_state = np.random.choice(states, p=transition_mat[prev_state])\n",
    "        state_list[n] = state2ix[next_state]\n",
    "        next_data = np.random.choice([0, 1, 2, 3, 4, 5], p=probabilities[next_state])\n",
    "        data[n] = next_data\n",
    "        prev_state = next_state\n",
    "    return data, state_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 15\n",
    "rolls = np.zeros((5000, n_obs)).astype(int)\n",
    "targets = np.zeros((5000, n_obs)).astype(int)\n",
    "for i in range(5000):\n",
    "    data, dices = simulate_data(n_obs)\n",
    "    rolls[i] = data.reshape(1, -1).astype(int)\n",
    "    targets[i] = dices.reshape(1, -1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19293/3618723230.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"./models/crf.hdf5\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CRF(2, log_likelihood)\n",
    "model = crf_train_loop(model, rolls, targets, 10, 0.001)\n",
    "torch.save(model.state_dict(), \"./models/crf.hdf5\")\n",
    "model.load_state_dict(torch.load(\"./models/crf.hdf5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 4 1 3 4 1 2 4 0 5 5 5 5 5 5]\n",
      "[1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 1 1 1 1 1 1]\n",
      "[[0.6096541  0.18964128 0.33120134]\n",
      " [0.22679496 0.48702103 0.42212826]]\n"
     ]
    }
   ],
   "source": [
    "data, dices = simulate_data(15)\n",
    "test_rolls = data.reshape(1, -1).astype(int)\n",
    "test_targets = dices.reshape(1, -1).astype(int)\n",
    "print(test_rolls[0])\n",
    "print(model.forward(test_rolls[0])[0])\n",
    "print(test_targets[0])\n",
    "print(np.exp(list(model.parameters())[0].data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNA solving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define transition probabilities and emission probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = {\n",
    "    'H': {'H': 0.5, 'L': 0.5},\n",
    "    'L': {'H': 0.4, 'L': 0.6}\n",
    "}\n",
    "\n",
    "emission_probabilities = {\n",
    "    'H': {'A': 0.2, 'C': 0.3, 'G': 0.3, 'T': 0.2},\n",
    "    'L': {'A': 0.3, 'C': 0.2, 'G': 0.2, 'T': 0.3}\n",
    "}\n",
    "\n",
    "states = ['H', 'L']\n",
    "observations = ['G', 'C', 'A', 'C', 'T', 'G', 'A', 'A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Viterbi algorithm for hidden state prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algorithm(obs, states, start_prob, trans_prob, emit_prob):\n",
    "    n = len(obs)\n",
    "    viterbi = np.zeros((len(states), n))\n",
    "    path = np.zeros((len(states), n), dtype=int)\n",
    "\n",
    "    for s in range(len(states)):\n",
    "        viterbi[s, 0] = start_prob[states[s]] * emit_prob[states[s]][obs[0]]\n",
    "\n",
    "    for t in range(1, n):\n",
    "        for s in range(len(states)):\n",
    "            max_prob, max_state = max(\n",
    "                (viterbi[s_prev, t - 1] * trans_prob[states[s_prev]][states[s]] * emit_prob[states[s]][obs[t]], s_prev)\n",
    "                for s_prev in range(len(states))\n",
    "            )\n",
    "            viterbi[s, t] = max_prob\n",
    "            path[s, t] = max_state\n",
    "\n",
    "    max_final_prob, max_final_state = max((viterbi[s, n - 1], s) for s in range(len(states)))\n",
    "\n",
    "    optimal_path = [max_final_state]\n",
    "    for t in range(n - 1, 0, -1):\n",
    "        optimal_path.insert(0, path[optimal_path[0], t])\n",
    "\n",
    "    return max_final_prob, [states[state] for state in optimal_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable sequence of hidden states: ['H', 'H', 'L', 'L', 'L', 'L', 'L', 'L']\n",
      "Probability of the sequence: 2.834351999999999e-07\n"
     ]
    }
   ],
   "source": [
    "# Define start probabilities\n",
    "start_probabilities = {'H': 0.5, 'L': 0.5}\n",
    "\n",
    "# Convert observations to indices\n",
    "observations_indices = observations\n",
    "\n",
    "# Run the Viterbi algorithm\n",
    "max_probability, hidden_states = viterbi_algorithm(\n",
    "    observations_indices, states, start_probabilities, transition_probabilities, emission_probabilities\n",
    ")\n",
    "\n",
    "print(\"Most probable sequence of hidden states:\", hidden_states)\n",
    "print(\"Probability of the sequence:\", max_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель CRF показала ймовірності переходів між станами і на основі послідовності спрогнозувала, які стани (зміни чи сталість типу кубика) найімовірніші на кожному кроці. Результат демонструє високу відповідність моделі завданням, пов’язаним із залежностями між мітками. У випадку з ДНК, алгоритм Вітербі передбачив, що більша частина послідовності має стан \"L\" (низький вміст GC), з початковим високим вмістом GC (\"H\"). Хоча ймовірність результату низька, це може бути обумовлено природними обмеженнями вибірки та ймовірнісної моделі.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Контрольні запитання\n",
    "1. У чому суть Conditional Random Fields?\n",
    "2. Що являє собою алгоритм Viterbi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. CRF — це ймовірнісна модель для прогнозування послідовності міток, яка враховує залежність між сусідніми мітками. Вона дозволяє знаходити найбільш імовірну комбінацію міток для заданих даних.\n",
    "2. Вітербі — це динамічний алгоритм для знаходження найімовірнішої послідовності прихованих станів у задачах із прихованими марковськими моделями (HMM). Він враховує ймовірності переходів і емісій для побудови оптимального шляху."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
