{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Компʼютерний практикум №16\n",
    "Виконав студент групи ЗК-41мп Гломозда Костянтин\n",
    "\n",
    "РЕКУРЕНТНІ МЕРЕЖІ (RNN, GRU, LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device configuration and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data/mnist',\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data/mnist',\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the RNN model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.3566\n",
      "Epoch [2/10], Loss: 2.2502\n",
      "Epoch [3/10], Loss: 2.3491\n",
      "Epoch [4/10], Loss: 2.4235\n",
      "Epoch [5/10], Loss: 2.3818\n",
      "Epoch [6/10], Loss: 2.3251\n",
      "Epoch [7/10], Loss: 2.3482\n",
      "Epoch [8/10], Loss: 2.3759\n",
      "Epoch [9/10], Loss: 2.6778\n",
      "Epoch [10/10], Loss: 2.3740\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "  \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 9.82 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy of the model on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [4 4 4 4 4 4 4 4 4 4]\n",
      "Labels: [7 2 1 0 4 1 4 9 5 9]\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'models/rnn_model_1.ckpt')\n",
    "\n",
    "# Print 10 predictions from test data\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(\"Predicted:\", predicted[:10].cpu().numpy())\n",
    "        print(\"Labels:\", labels[:10].cpu().numpy())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0750\n",
      "Epoch [2/10], Loss: 0.1020\n",
      "Epoch [3/10], Loss: 0.0304\n",
      "Epoch [4/10], Loss: 0.0302\n",
      "Epoch [5/10], Loss: 0.0153\n",
      "Epoch [6/10], Loss: 0.0187\n",
      "Epoch [7/10], Loss: 0.0235\n",
      "Epoch [8/10], Loss: 0.0178\n",
      "Epoch [9/10], Loss: 0.0194\n",
      "Epoch [10/10], Loss: 0.0139\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 256  # Increased hidden size for better feature extraction\n",
    "num_layers = 3  # Added more layers for depth\n",
    "num_classes = 10\n",
    "batch_size = 128  # Increased batch size for faster convergence\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001  # Lowered learning rate for stable training\n",
    "dropout_rate = 0.2  # Added dropout to prevent overfitting\n",
    "\n",
    "# MNIST dataset loading and preprocessing\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data/mnist',\n",
    "    train=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize input for better training\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data/mnist',\n",
    "    train=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize input for better training\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class OptimizedRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_rate):\n",
    "        super(OptimizedRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.rnn(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Model initialization\n",
    "model = OptimizedRNN(input_size, hidden_size, num_layers, num_classes, dropout_rate).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 99.04 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy of the model on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [7 2 1 0 4 1 4 9 5 9]\n",
      "Labels: [7 2 1 0 4 1 4 9 5 9]\n"
     ]
    }
   ],
   "source": [
    "# Saving and testing the model\n",
    "torch.save(model.state_dict(), 'models/optimized_rnn_model.ckpt')\n",
    "\n",
    "# Print 10 predictions from test data\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(\"Predicted:\", predicted[:10].cpu().numpy())\n",
    "        print(\"Labels:\", labels[:10].cpu().numpy())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GRU architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0494\n",
      "Epoch [2/10], Loss: 0.1136\n",
      "Epoch [3/10], Loss: 0.0163\n",
      "Epoch [4/10], Loss: 0.0627\n",
      "Epoch [5/10], Loss: 0.0093\n",
      "Epoch [6/10], Loss: 0.0036\n",
      "Epoch [7/10], Loss: 0.0046\n",
      "Epoch [8/10], Loss: 0.1276\n",
      "Epoch [9/10], Loss: 0.0033\n",
      "Epoch [10/10], Loss: 0.0105\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Device configuration and hyperparameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 256\n",
    "num_layers = 3\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# MNIST dataset loading and preprocessing\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data/mnist',\n",
    "    train=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data/mnist',\n",
    "    train=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the GRU-based model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_rate):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = GRUModel(input_size, hidden_size, num_layers, num_classes, dropout_rate).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the GRU model on the 10000 test images: 99.11 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy of the GRU model on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [7 2 1 0 4 1 4 9 5 9]\n",
      "Labels: [7 2 1 0 4 1 4 9 5 9]\n"
     ]
    }
   ],
   "source": [
    "# Saving and testing the model\n",
    "torch.save(model.state_dict(), 'models/gru_rnn.ckpt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(\"Predicted:\", predicted[:10].cpu().numpy())\n",
    "        print(\"Labels:\", labels[:10].cpu().numpy())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LTSM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1176\n",
      "Epoch [2/10], Loss: 0.0557\n",
      "Epoch [3/10], Loss: 0.0419\n",
      "Epoch [4/10], Loss: 0.0537\n",
      "Epoch [5/10], Loss: 0.0753\n",
      "Epoch [6/10], Loss: 0.0613\n",
      "Epoch [7/10], Loss: 0.0103\n",
      "Epoch [8/10], Loss: 0.0247\n",
      "Epoch [9/10], Loss: 0.0049\n",
      "Epoch [10/10], Loss: 0.0736\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Device configuration and hyperparameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 256\n",
    "num_layers = 3\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# MNIST dataset loading and preprocessing\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data/mnist',\n",
    "    train=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data/mnist',\n",
    "    train=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the LSTM-based model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_rate):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes, dropout_rate).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the LSTM model on the 10000 test images: 98.77 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy of the LSTM model on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [7 2 1 0 4 1 4 9 5 9]\n",
      "Labels: [7 2 1 0 4 1 4 9 5 9]\n"
     ]
    }
   ],
   "source": [
    "# Saving and testing the model\n",
    "torch.save(model.state_dict(), 'models/lstm_model_rnn.ckpt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(\"Predicted:\", predicted[:10].cpu().numpy())\n",
    "        print(\"Labels:\", labels[:10].cpu().numpy())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: \n",
    "1. With improved custom RNN there was achieved 99% accuracy.\n",
    "2. GRU architecture has also achieved 99% accuracy but way faster\n",
    "3. LSTM achieved slightly less accuracy with the same time as GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will stick to GRU architecture and adapt it for CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [02:02<00:00, 1.39MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10\n",
      "Epoch [1/20], Loss: 1.7251\n",
      "Epoch [2/20], Loss: 1.1145\n",
      "Epoch [3/20], Loss: 1.2840\n",
      "Epoch [4/20], Loss: 1.1281\n",
      "Epoch [5/20], Loss: 0.8262\n",
      "Epoch [6/20], Loss: 0.8169\n",
      "Epoch [7/20], Loss: 0.8288\n",
      "Epoch [8/20], Loss: 0.4626\n",
      "Epoch [9/20], Loss: 0.3677\n",
      "Epoch [10/20], Loss: 0.3729\n",
      "Epoch [11/20], Loss: 0.5157\n",
      "Epoch [12/20], Loss: 0.3552\n",
      "Epoch [13/20], Loss: 0.3423\n",
      "Epoch [14/20], Loss: 0.1857\n",
      "Epoch [15/20], Loss: 0.2823\n",
      "Epoch [16/20], Loss: 0.2608\n",
      "Epoch [17/20], Loss: 0.3177\n",
      "Epoch [18/20], Loss: 0.3515\n",
      "Epoch [19/20], Loss: 0.4774\n",
      "Epoch [20/20], Loss: 0.2757\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Device configuration and hyperparameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sequence_length = 32  # Treat each row of the image as a time step\n",
    "input_size = 32       # Each row has 32 pixel values (1D input per step)\n",
    "hidden_size = 512     # Increased hidden size for CIFAR10 complexity\n",
    "num_layers = 3        # Maintain depth for better feature extraction\n",
    "num_classes = 10      # CIFAR10 has 10 output classes\n",
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.3    # Slightly higher dropout for regularization\n",
    "\n",
    "# CIFAR10 dataset loading and preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale for simplicity\n",
    "    transforms.Resize((32, 32)),  # Ensure consistent size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize input\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data/cifar10',\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data/cifar10',\n",
    "    train=False,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the GRU-based model for CIFAR10\n",
    "class GRUCIFAR(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_rate):\n",
    "        super(GRUCIFAR, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = GRUCIFAR(input_size, hidden_size, num_layers, num_classes, dropout_rate).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        images = images.view(-1, sequence_length, input_size).to(device)  # Reshape for GRU\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the GRU model on CIFAR10: 59.98 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy of the GRU model on CIFAR10: {100 * correct / total:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [4 1 8 8 6 6 8 3 3 1]\n",
      "Labels: [3 8 8 0 6 6 1 6 3 1]\n"
     ]
    }
   ],
   "source": [
    "# Saving the model\n",
    "torch.save(model.state_dict(), 'models/gru_cifar10_model_rnn.ckpt')\n",
    "\n",
    "# Displaying predictions\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, sequence_length, input_size).to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(\"Predicted:\", predicted[:10].cpu().numpy())\n",
    "        print(\"Labels:\", labels[:10].cpu().numpy())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is worse than in previous labs (>90%). Let's try few optimization for CIFAR100 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar100/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [02:09<00:00, 1.30MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar100/cifar-100-python.tar.gz to ./data/cifar100/\n",
      "Epoch [1/30], Loss: 3.6883\n",
      "Epoch [2/30], Loss: 3.4932\n",
      "Epoch [3/30], Loss: 3.3434\n",
      "Epoch [4/30], Loss: 2.8912\n",
      "Epoch [5/30], Loss: 2.6317\n",
      "Epoch [6/30], Loss: 2.9532\n",
      "Epoch [7/30], Loss: 2.5863\n",
      "Epoch [8/30], Loss: 2.4893\n",
      "Epoch [9/30], Loss: 2.3768\n",
      "Epoch [10/30], Loss: 2.2451\n",
      "Epoch [11/30], Loss: 2.0422\n",
      "Epoch [12/30], Loss: 1.6040\n",
      "Epoch [13/30], Loss: 1.7357\n",
      "Epoch [14/30], Loss: 1.2323\n",
      "Epoch [15/30], Loss: 0.9799\n",
      "Epoch [16/30], Loss: 1.0345\n",
      "Epoch [17/30], Loss: 0.8323\n",
      "Epoch [18/30], Loss: 0.6156\n",
      "Epoch [19/30], Loss: 0.6901\n",
      "Epoch [20/30], Loss: 0.6305\n",
      "Epoch [21/30], Loss: 0.4033\n",
      "Epoch [22/30], Loss: 0.4221\n",
      "Epoch [23/30], Loss: 0.4783\n",
      "Epoch [24/30], Loss: 0.3901\n",
      "Epoch [25/30], Loss: 0.4003\n",
      "Epoch [26/30], Loss: 0.4562\n",
      "Epoch [27/30], Loss: 0.3241\n",
      "Epoch [28/30], Loss: 0.4736\n",
      "Epoch [29/30], Loss: 0.3211\n",
      "Epoch [30/30], Loss: 0.3614\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Device configuration and hyperparameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sequence_length = 32\n",
    "input_size = 32\n",
    "hidden_size = 512\n",
    "num_layers = 4  # Increased depth for CIFAR100 complexity\n",
    "num_classes = 100  # CIFAR100 has 100 output classes\n",
    "batch_size = 128\n",
    "num_epochs = 30  # Increased epochs for better convergence\n",
    "learning_rate = 0.0005  # Lowered learning rate for stability\n",
    "dropout_rate = 0.4  # Higher dropout rate to prevent overfitting\n",
    "\n",
    "# CIFAR100 dataset loading and preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale for simplicity\n",
    "    transforms.Resize((32, 32)),  # Ensure consistent size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize input\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR100(\n",
    "    root='./data/cifar100/',\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR100(\n",
    "    root='./data/cifar100/',\n",
    "    train=False,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the GRU-based model for CIFAR100\n",
    "class GRUCIFAR100(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_rate):\n",
    "        super(GRUCIFAR100, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = GRUCIFAR100(input_size, hidden_size, num_layers, num_classes, dropout_rate).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)  # AdamW for better regularization\n",
    "\n",
    "# Training the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        images = images.view(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the GRU model on CIFAR100: 28.09 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy of the GRU model on CIFAR100: {100 * correct / total:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [79 70 51 55 71 50 88 66 60 34]\n",
      "Labels: [49 33 72 51 71 92 15 14 23  0]\n"
     ]
    }
   ],
   "source": [
    "# Saving the model\n",
    "torch.save(model.state_dict(), 'models/gru_cifar100_model_rnn.ckpt')\n",
    "\n",
    "# Displaying predictions\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, sequence_length, input_size).to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(\"Predicted:\", predicted[:10].cpu().numpy())\n",
    "        print(\"Labels:\", labels[:10].cpu().numpy())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, after several tests there wasn't achieve such a good accuracy as in previous labs. Maybe it can be done using LTSM with other hyperparameters for better optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "Оптимізація моделі для CIFAR100 передбачала збільшення глибини (4 шари), розмірів прихованого стану (512 нейронів), а також додавання регуляризації через Dropout (0.4) та оптимізатор AdamW. Такі параметри були обрані для врахування складності задачі з 100 класами. Однак, перехід до одноканальних (Grayscale) даних, збереження тільки часових залежностей та відсутність попередньої обробки, зокрема згорткових шарів (CNN), обмежили точність. Результат у 28.09% свідчить, що GRU без додаткового врахування просторових ознак не може повністю розкрити інформацію у зображеннях CIFAR100, і майбутні покращення варто шукати в поєднанні RNN та CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Що являє собою рекурентна нейронна мережа?\n",
    "2. Які різновиди рекурентних нейронних мереж існують?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Рекурентна нейронна мережа (RNN) — це модель штучного інтелекту, яка обробляє послідовні дані, використовуючи прихований стан для збереження інформації з попередніх часових кроків. Це дозволяє їй враховувати контекст у задачах, таких як обробка мови, часові ряди або послідовні дані.\n",
    "2. Існують базові RNN, а також покращені версії для вирішення проблеми зникаючих/вибухаючих градієнтів: LSTM (Long Short-Term Memory) та GRU (Gated Recurrent Unit). Додатково існують моделі з двонаправленими зв'язками (Bidirectional RNN) та розширення, як Attention та Transformer, які поєднують RNN із сучасними підходами."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
